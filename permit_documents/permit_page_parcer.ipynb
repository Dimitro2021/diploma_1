{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3166a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import re\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import ast\n",
    "import csv\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from math import ceil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8eca1",
   "metadata": {},
   "source": [
    "### Permit Data Register Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1daea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏\n",
    "BASE_URL = \"https://e-construction.gov.ua/document/optype=100/filter=780_2024-01-01_2024-12-31\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept-Language\": \"uk-UA,uk;q=0.9\"\n",
    "}\n",
    "\n",
    "def extract_permit_doc_ids(html, page_num=0):\n",
    "    \"\"\"\n",
    "    Extracts permit document IDs and registration numbers from the HTML content of a page.\n",
    "    \n",
    "    Args:\n",
    "        html (str): The HTML content of the page.\n",
    "        page_num (int): The page number being processed.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of tuples containing doc_id, registration_number, and page_num.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = []\n",
    "\n",
    "    items = soup.select(\"div.dataset__item\")\n",
    "    if len(items) < 10:\n",
    "        tqdm.write(f\"‚ö†Ô∏è Warning: only {len(items)} items found on page {page_num}\")\n",
    "\n",
    "    for item in items:\n",
    "        link = item.select_one(\"a.btn.btn-primary[href*='doc_id='][href*='optype=100']\")\n",
    "        doc_match = re.search(r\"doc_id=(\\d+)\", link[\"href\"]) if link else None\n",
    "        doc_id = doc_match.group(1) if doc_match else None\n",
    "\n",
    "        name_tag = item.select_one(\"h3.opendata__name\")\n",
    "        registration_number = name_tag.text.strip() if name_tag else \"\"\n",
    "\n",
    "        if doc_id:\n",
    "            results.append((doc_id, registration_number, page_num))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fetch_permit_page(page, session, headers):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a specific permit page.\n",
    "    \n",
    "    Args:\n",
    "        page (int): The page number to fetch.\n",
    "        session (requests.Session): The session object for making HTTP requests.\n",
    "        headers (dict): The HTTP headers to use for the request.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the page number and the HTML content (or None if failed).\n",
    "    \"\"\"\n",
    "    url = BASE_URL if page == 1 else f\"{BASE_URL}/page={page}\"\n",
    "    headers[\"Referer\"] = url\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return page, response.text\n",
    "    except requests.RequestException:\n",
    "        return page, None\n",
    "\n",
    "def scrape_permit_docs(start_page=1, end_page=1000, save_interval=1000, output_file=\"permit_documents.csv\", max_workers=10):\n",
    "    \"\"\"\n",
    "    Scrapes permit documents from multiple pages and saves the data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        start_page (int): The starting page number.\n",
    "        end_page (int): The ending page number.\n",
    "        save_interval (int): The number of records to save in each batch.\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        max_workers (int): The maximum number of threads to use for concurrent requests.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of pages that were skipped during scraping.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    skipped_pages = []\n",
    "    column_names = [\"doc_id\", \"registration_number_edessb\", \"page_num\"]\n",
    "\n",
    "    session = requests.Session()\n",
    "    pages_parsed = 0\n",
    "    pages = set(range(start_page, end_page + 1))\n",
    "    try:\n",
    "        existing_data = pd.read_csv(output_file)\n",
    "        parsed_pages = set(existing_data[\"page_num\"].unique())\n",
    "        tqdm.write(f\"‚ö° Already parsed {len(parsed_pages)}\")\n",
    "        pages = pages-parsed_pages\n",
    "    except (IndexError, FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        tqdm.write(\"üîπ Starting from scratch!\")\n",
    "        pd.DataFrame(columns=column_names).to_csv(output_file, index=False)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_page = {executor.submit(fetch_permit_page, page, session, HEADERS): page for page in pages}\n",
    "\n",
    "        for future in tqdm(as_completed(future_to_page), total=len(future_to_page), desc=\"üìÑ Scraping pages\"):\n",
    "            page = future_to_page[future]\n",
    "            try:\n",
    "                page, html = future.result()\n",
    "                if html is None:\n",
    "                    skipped_pages.append(page)\n",
    "                    continue\n",
    "\n",
    "                data = extract_permit_doc_ids(html, page_num=page)\n",
    "                all_data.extend(data)\n",
    "                pages_parsed += 1\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"‚ö†Ô∏è Error processing page {page}: {e}\")\n",
    "                skipped_pages.append(page)\n",
    "\n",
    "            if len(all_data) >= save_interval * 12:\n",
    "                df = pd.DataFrame(all_data, columns=column_names)\n",
    "                df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "                tqdm.write(f\"üíæ Saved batch at page {page}\")\n",
    "                all_data = []\n",
    "\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data, columns=column_names)\n",
    "        df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "        tqdm.write(f\"‚úÖ Final save of {len(all_data)} records.\")\n",
    "\n",
    "    tqdm.write(f\"üîö Completed! Skipped pages: {len(skipped_pages)}\")\n",
    "\n",
    "    # Retry skipped\n",
    "    still_skipped_pages = []\n",
    "    if skipped_pages:\n",
    "        tqdm.write(\"üîÑ Retrying failed pages...\")\n",
    "        for sk_page in tqdm(skipped_pages, desc=\"üîÅ Retrying\"):\n",
    "            page, html = fetch_permit_page(sk_page, session, HEADERS)\n",
    "            if html is None:\n",
    "                tqdm.write(f\"‚ùå Failed to retry page {sk_page}\")\n",
    "                still_skipped_pages.append(sk_page)\n",
    "                continue\n",
    "            data = extract_permit_doc_ids(html, page_num=sk_page)\n",
    "            all_data.extend(data)\n",
    "            df = pd.DataFrame(all_data, columns=column_names)\n",
    "            df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "            all_data = []\n",
    "\n",
    "    tqdm.write(f\"üîö Final skipped pages: {len(still_skipped_pages)}\")\n",
    "    return still_skipped_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffae4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_permit_docs(end_page=11472, output_file=\"permit_documents_3.csv\", max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fdbcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse register 3 times to get all data\n",
    "df_1 = pd.read_csv('permit_documents_1.csv')\n",
    "df_2 = pd.read_csv('permit_documents_2.csv')\n",
    "df_3 = pd.read_csv('permit_documents_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19ef9f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique doc_id across all: 132852\n",
      "Shared between all three: 44162\n",
      "Unique to df_1: 8985\n",
      "Unique to df_2: 9530\n",
      "Unique to df_3: 9762\n"
     ]
    }
   ],
   "source": [
    "# Convert to sets for comparison\n",
    "doc_ids_df1 = set(df_1['doc_id'].dropna())\n",
    "doc_ids_df2 = set(df_2['doc_id'].dropna())\n",
    "doc_ids_df3 = set(df_3['doc_id'].dropna())\n",
    "\n",
    "\n",
    "# Total unique doc_id across all three\n",
    "all_unique_doc_ids = len(doc_ids_df1 | doc_ids_df2 | doc_ids_df3)\n",
    "print(\"Total unique doc_id across all:\", all_unique_doc_ids)\n",
    "\n",
    "# Shared in all three\n",
    "shared_all = doc_ids_df1 & doc_ids_df2 & doc_ids_df3\n",
    "print(\"Shared between all three:\", len(shared_all))\n",
    "\n",
    "# Unique to each\n",
    "print(\"Unique to df_1:\", len(doc_ids_df1 - doc_ids_df2 - doc_ids_df3))\n",
    "print(\"Unique to df_2:\", len(doc_ids_df2 - doc_ids_df1 - doc_ids_df3))\n",
    "print(\"Unique to df_3:\", len(doc_ids_df3 - doc_ids_df1 - doc_ids_df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "828377f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "almost_all_ids = pd.DataFrame({'doc_id': list(doc_ids_df1 | doc_ids_df2 | doc_ids_df3)})\n",
    "\n",
    "# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É CSV\n",
    "# almost_all_ids.to_csv('permit_ids_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1112b1",
   "metadata": {},
   "source": [
    "### Permit Document Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc634b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_old = 'https://e-construction.gov.ua/document_detail/doc_id=2634101501580019683/optype=100'\n",
    "url_new = 'https://e-construction.gov.ua/document_detail/doc_id=3295860232785233181/optype=100'\n",
    "url = 'https://e-construction.gov.ua/document_detail/doc_id=3272199483311523749/optype=100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74427eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept-Language\": \"uk-UA,uk;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parsing the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1243fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing extra whitespace and replacing certain characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    return text.strip().replace(\";\", \",\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "def extract_named_tables(soup):\n",
    "    \"\"\"\n",
    "    Extracts specific tables from the HTML soup based on predefined titles.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted table data.\n",
    "    \"\"\"\n",
    "    table_titles = [\n",
    "        \"–í—ñ–¥–æ–º–æ—Å—Ç—ñ –ø—Ä–æ –∑–∞–º–æ–≤–Ω–∏–∫–∞\",\n",
    "        \"–ó–∞–º–æ–≤–Ω–∏–∫–∏, —è–∫—ñ –¥–µ–ª–µ–≥—É–≤–∞–ª–∏ —Å–≤–æ—ó –ø–æ–≤–Ω–æ–≤–∞–∂–µ–Ω–Ω—è\",\n",
    "        \"–î–æ–∫—É–º–µ–Ω—Ç, –Ω–∞ –æ—Å–Ω–æ–≤—ñ —è–∫–æ–≥–æ —Ä–µ—î—Å—Ç—Ä—É—î—Ç—å—Å—è –¥–µ–∫–ª–∞—Ä–∞—Ü—ñ—è\",\n",
    "        \"–¢–µ—Ä–º—ñ–Ω–∏ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\"\n",
    "    ]\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for title in table_titles:\n",
    "        h3 = soup.find(\"h3\", string=lambda x: x and title in x)\n",
    "        if not h3:\n",
    "            continue\n",
    "\n",
    "        next_div = h3.find_next_sibling()\n",
    "        if next_div and \"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –∑–∞–∑–Ω–∞—á–µ–Ω–æ\" in next_div.get_text():\n",
    "            continue\n",
    "\n",
    "        table = h3.find_next(\"table\")\n",
    "        if not table:\n",
    "            continue\n",
    "\n",
    "        thead = table.find(\"thead\")\n",
    "        tbody = table.find(\"tbody\")\n",
    "        if not thead or not tbody:\n",
    "            continue\n",
    "\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "        first_row = tbody.find(\"tr\")\n",
    "        if not first_row:\n",
    "            continue\n",
    "\n",
    "        values = [td.get_text(strip=True) for td in first_row.find_all(\"td\")]\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        for h, v in zip(headers, values):\n",
    "            result_dict[f\"{title}_{clean_text(h)}\"] = clean_text(v)\n",
    "\n",
    "    return pd.DataFrame([result_dict])\n",
    "\n",
    "def flatten_permit_data(parsed_dict):\n",
    "    \"\"\"\n",
    "    Flattens a nested dictionary of permit data into a single-row DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parsed_dict (dict): Nested dictionary of parsed permit data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with flattened data.\n",
    "    \"\"\"\n",
    "    flat_row = {}\n",
    "    for section, entries in parsed_dict.items():\n",
    "        if not entries:\n",
    "            continue\n",
    "        for entry in entries:\n",
    "            data = entry.get('data', {})\n",
    "            for key, value in data.items():\n",
    "                col_name = f\"{key}_{section}\"\n",
    "                flat_row[clean_text(col_name)] = clean_text(value)\n",
    "    return pd.DataFrame([flat_row])\n",
    "\n",
    "def parse_sections_with_subsections(soup):\n",
    "    \"\"\"\n",
    "    Parses sections and subsections from the HTML soup.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing sections and their subsections with data.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for header in soup.select('h3.object-title'):\n",
    "        section_name = clean_text(header.get_text())\n",
    "        subsections = []\n",
    "        # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ‚Äî —Å–∞–º–µ —Ä–æ–∑–¥—ñ–ª (–ø–æ–∫–∏ –Ω–µ –∑—É—Å—Ç—Ä—ñ–ª–∏ <h5>)\n",
    "        current_sub = None\n",
    "        current_data = {}\n",
    "        \n",
    "        for elem in header.next_elements:\n",
    "            # –Ø–∫—â–æ –Ω–∞—Ç—Ä–∞–ø–∏–ª–∏ –Ω–∞ –Ω–æ–≤–∏–π —Ä–æ–∑–¥—ñ–ª ‚Äî –∑—É–ø–∏–Ω—è—î–º–æ—Å—è\n",
    "            if isinstance(elem, Tag) and elem.name == 'h3' and 'object-title' in elem.get('class', []):\n",
    "                break\n",
    "            # –Ø–∫—â–æ –Ω–∞—Ç—Ä–∞–ø–∏–ª–∏ –Ω–∞ –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª <h5> ‚Äî –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –±–ª–æ–∫\n",
    "            if isinstance(elem, Tag) and elem.name == 'h5':\n",
    "                # –∑–±–µ—Ä–µ–≥—Ç–∏, —è–∫—â–æ —î –∑—ñ–±—Ä–∞–Ω—ñ –ø–æ–ª—è\n",
    "                if current_data:\n",
    "                    subsections.append({\n",
    "                        'subsection': current_sub,\n",
    "                        'data': current_data\n",
    "                    })\n",
    "                current_sub = clean_text(elem.get_text())\n",
    "                current_data = {}\n",
    "            # –Ø–∫—â–æ —Ü–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–π –ø—É–Ω–∫—Ç ‚Äî –¥–æ–¥–∞—î–º–æ –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è\n",
    "            if isinstance(elem, Tag) and 'object-info-item' in elem.get('class', []):\n",
    "                left = elem.select_one('.object-info_left')\n",
    "                right = elem.select_one('.object-info_right')\n",
    "                if left and right:\n",
    "                    key = clean_text(left.get_text())\n",
    "                    value = clean_text(right.get_text())\n",
    "                    current_data[key] = value\n",
    "        \n",
    "        # –ó–±–µ—Ä–µ–≥—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π –±–ª–æ–∫\n",
    "        if current_data:\n",
    "            subsections.append({\n",
    "                'subsection': current_sub,\n",
    "                'data': current_data\n",
    "            })\n",
    "        \n",
    "        result[section_name] = subsections\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_fancytree_blocks(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts JavaScript blocks related to FancyTree from the HTML soup.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of extracted JavaScript blocks.\n",
    "    \"\"\"\n",
    "    full_text = soup.decode()\n",
    "    blocks = re.findall(\n",
    "        r'head\\.load\\(\\[.*?fancytree\\.bundle\\.js.*?\\],function\\(\\)\\{(.*?)\\}\\);',\n",
    "        full_text,\n",
    "        re.DOTALL\n",
    "    )\n",
    "    return blocks\n",
    "\n",
    "def extract_parent_doc_id(js_text: str):\n",
    "    \"\"\"\n",
    "    Extracts the parent document ID from a JavaScript block.\n",
    "\n",
    "    Args:\n",
    "        js_text (str): JavaScript block as a string.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted parent document ID, or None if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\"obj_comp_id\"\\s*:\\s*\"(\\d+)\"', js_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def final_function(soup):\n",
    "    \"\"\"\n",
    "    Combines multiple parsing functions to extract and merge permit data.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined permit data.\n",
    "    \"\"\"\n",
    "    info_table = extract_named_tables(soup)\n",
    "    sections = parse_sections_with_subsections(soup)\n",
    "    info_sections = flatten_permit_data(sections)\n",
    "    blocks = extract_fancytree_blocks(soup)\n",
    "    info_tep = {}\n",
    "    for i, block in enumerate(blocks):\n",
    "        info_tep[f\"block_{i}\"] = extract_parent_doc_id(block)\n",
    "\n",
    "    # –ó'—î–¥–Ω—É—î–º–æ —Ç–∞–±–ª–∏—Ü—ñ info_table —Ç–∞ info_sections –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—ñ\n",
    "    base_df = pd.concat([info_table, info_sections], axis=1)\n",
    "\n",
    "    # –í–∏—Ç—è–≥—É—î–º–æ –≤—Å—ñ obj_comp_id —É —Å–ø–∏—Å–æ–∫\n",
    "    tep_ids = list(filter(None, info_tep.values()))\n",
    "    base_df[\"teps\"] = [tep_ids]\n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "943b3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_function(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933c17c",
   "metadata": {},
   "source": [
    "### TEP Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a865ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://e-construction.gov.ua/document_detail_tep/doc_id=2895385515312285052\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edf66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept-Language\": \"uk-UA,uk;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parsing the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59337e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tep_info(soup):\n",
    "    # –û—Å–Ω–æ–≤–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è\n",
    "    basic_info = []\n",
    "    for item in soup.select('.object-info-item'):\n",
    "        label = item.select_one('.object-info_left span')\n",
    "        value = item.select_one('.object-info_right span')\n",
    "        if label and value:\n",
    "            basic_info.append({\n",
    "                '–†–æ–∑–¥—ñ–ª': '–û—Å–Ω–æ–≤–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –æ–±‚Äô—î–∫—Ç',\n",
    "                '–ù–∞–∑–≤–∞': label.text.strip(),\n",
    "                '–ó–Ω–∞—á–µ–Ω–Ω—è': value.text.strip()\n",
    "            })\n",
    "\n",
    "    # –¢–ï–ü\n",
    "    tep_info = []\n",
    "    tep_table = soup.select_one('#tep + div table')\n",
    "    if tep_table:\n",
    "        for row in tep_table.select('tbody tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) >= 4:\n",
    "                tep_info.append({\n",
    "                    '–†–æ–∑–¥—ñ–ª': '–¢–µ—Ö–Ω—ñ–∫–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏',\n",
    "                    '–ù–∞–∑–≤–∞': cols[1].text.strip(),\n",
    "                    '–ó–Ω–∞—á–µ–Ω–Ω—è': cols[3].text.strip(),\n",
    "                    '–ü—Ä–∏–º—ñ—Ç–∫–∞': cols[4].text.strip() if len(cols) > 4 else ''\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(basic_info + tep_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc95e1f2",
   "metadata": {},
   "source": [
    "### Get Permit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99277a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://e-construction.gov.ua/document_detail/doc_id={}/optype=100\"\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().replace(\";\", \",\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "def extract_named_tables(soup):\n",
    "    table_titles = [\n",
    "        \"–í—ñ–¥–æ–º–æ—Å—Ç—ñ –ø—Ä–æ –∑–∞–º–æ–≤–Ω–∏–∫–∞\",\n",
    "        \"–ó–∞–º–æ–≤–Ω–∏–∫–∏, —è–∫—ñ –¥–µ–ª–µ–≥—É–≤–∞–ª–∏ —Å–≤–æ—ó –ø–æ–≤–Ω–æ–≤–∞–∂–µ–Ω–Ω—è\",\n",
    "        \"–î–æ–∫—É–º–µ–Ω—Ç, –Ω–∞ –æ—Å–Ω–æ–≤—ñ —è–∫–æ–≥–æ —Ä–µ—î—Å—Ç—Ä—É—î—Ç—å—Å—è –¥–µ–∫–ª–∞—Ä–∞—Ü—ñ—è\",\n",
    "        \"–¢–µ—Ä–º—ñ–Ω–∏ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\"\n",
    "    ]\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for title in table_titles:\n",
    "        h3 = soup.find(\"h3\", string=lambda x: x and title in x)\n",
    "        if not h3:\n",
    "            continue\n",
    "\n",
    "        next_div = h3.find_next_sibling()\n",
    "        if next_div and \"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –∑–∞–∑–Ω–∞—á–µ–Ω–æ\" in next_div.get_text():\n",
    "            continue\n",
    "\n",
    "        table = h3.find_next(\"table\")\n",
    "        if not table:\n",
    "            continue\n",
    "\n",
    "        thead = table.find(\"thead\")\n",
    "        tbody = table.find(\"tbody\")\n",
    "        if not thead or not tbody:\n",
    "            continue\n",
    "\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "        first_row = tbody.find(\"tr\")\n",
    "        if not first_row:\n",
    "            continue\n",
    "\n",
    "        values = [td.get_text(strip=True) for td in first_row.find_all(\"td\")]\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        for h, v in zip(headers, values):\n",
    "            result_dict[f\"{title}_{clean_text(h)}\"] = clean_text(v)\n",
    "\n",
    "    return pd.DataFrame([result_dict])\n",
    "\n",
    "def flatten_permit_data(parsed_dict):\n",
    "    \"\"\"\n",
    "    –ü—Ä–∏–π–º–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º parse_sections_with_subsections,\n",
    "    –ø–æ–≤–µ—Ä—Ç–∞—î –¥–∞—Ç–∞—Ñ—Ä–µ–π–º: –ö–ª—é—á_–†–æ–∑–¥—ñ–ª\n",
    "    \"\"\"\n",
    "    flat_row = {}\n",
    "    for section, entries in parsed_dict.items():\n",
    "        if not entries:\n",
    "            continue\n",
    "        for entry in entries:\n",
    "            data = entry.get('data', {})\n",
    "            for key, value in data.items():\n",
    "                col_name = f\"{key}_{section}\"\n",
    "                flat_row[clean_text(col_name)] = clean_text(value)\n",
    "    return pd.DataFrame([flat_row])\n",
    "\n",
    "def parse_sections_with_subsections(soup):\n",
    "    \"\"\"\n",
    "    –ü–∞—Ä—Å–∏—Ç—å HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ñ–î–ï–°–°–ë —Ç–∞ –≤–∏—Ç—è–≥—É—î –¥–∞–Ω—ñ –∑ –∫–æ–∂–Ω–æ–≥–æ —Ä–æ–∑–¥—ñ–ª—É <h3>,\n",
    "    –≤–∫–ª—é—á–Ω–æ –∑ –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª–∞–º–∏ <h5>. –ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É:\n",
    "\n",
    "    {\n",
    "      \"–ù–∞–∑–≤–∞ —Ä–æ–∑–¥—ñ–ª—É\": [\n",
    "        {\n",
    "          \"subsection\": \"–ù–∞–∑–≤–∞ –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É –∞–±–æ None\",\n",
    "          \"data\": { key: value, ... }\n",
    "        },\n",
    "        ...\n",
    "      ],\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for header in soup.select('h3.object-title'):\n",
    "        section_name = clean_text(header.get_text())\n",
    "        subsections = []\n",
    "        # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª ‚Äî —Å–∞–º–µ —Ä–æ–∑–¥—ñ–ª (–ø–æ–∫–∏ –Ω–µ –∑—É—Å—Ç—Ä—ñ–ª–∏ <h5>)\n",
    "        current_sub = None\n",
    "        current_data = {}\n",
    "        \n",
    "        for elem in header.next_elements:\n",
    "            # –Ø–∫—â–æ –Ω–∞—Ç—Ä–∞–ø–∏–ª–∏ –Ω–∞ –Ω–æ–≤–∏–π —Ä–æ–∑–¥—ñ–ª ‚Äî –∑—É–ø–∏–Ω—è—î–º–æ—Å—è\n",
    "            if isinstance(elem, Tag) and elem.name == 'h3' and 'object-title' in elem.get('class', []):\n",
    "                break\n",
    "            # –Ø–∫—â–æ –Ω–∞—Ç—Ä–∞–ø–∏–ª–∏ –Ω–∞ –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª <h5> ‚Äî –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –±–ª–æ–∫\n",
    "            if isinstance(elem, Tag) and elem.name == 'h5':\n",
    "                # –∑–±–µ—Ä–µ–≥—Ç–∏, —è–∫—â–æ —î –∑—ñ–±—Ä–∞–Ω—ñ –ø–æ–ª—è\n",
    "                if current_data:\n",
    "                    subsections.append({\n",
    "                        'subsection': current_sub,\n",
    "                        'data': current_data\n",
    "                    })\n",
    "                current_sub = clean_text(elem.get_text())\n",
    "                current_data = {}\n",
    "            # –Ø–∫—â–æ —Ü–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–π –ø—É–Ω–∫—Ç ‚Äî –¥–æ–¥–∞—î–º–æ –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è\n",
    "            if isinstance(elem, Tag) and 'object-info-item' in elem.get('class', []):\n",
    "                left = elem.select_one('.object-info_left')\n",
    "                right = elem.select_one('.object-info_right')\n",
    "                if left and right:\n",
    "                    key = clean_text(left.get_text())\n",
    "                    value = clean_text(right.get_text())\n",
    "                    current_data[key] = value\n",
    "        \n",
    "        # –ó–±–µ—Ä–µ–≥—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π –±–ª–æ–∫\n",
    "        if current_data:\n",
    "            subsections.append({\n",
    "                'subsection': current_sub,\n",
    "                'data': current_data\n",
    "            })\n",
    "        \n",
    "        result[section_name] = subsections\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_fancytree_blocks(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    –í–∏—Ç—è–≥—É—î –≤—Å—ñ JS-–±–ª–æ–∫–∏ FancyTree –∑ HTML. –ë–ª–æ–∫–∏ –≤–∏–∑–Ω–∞—á–∞—é—Ç—å—Å—è –∑–∞ —Ñ–æ—Ä–º–æ—é:\n",
    "    head.load([...fancytree.bundle.js...], function() { ... });\n",
    "\n",
    "    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ –∫–æ–∂–Ω–∏–º –∑–Ω–∞–π–¥–µ–Ω–∏–º –±–ª–æ–∫–æ–º.\n",
    "    \"\"\"\n",
    "    full_text = soup.decode()\n",
    "    blocks = re.findall(\n",
    "        r'head\\.load\\(\\[.*?fancytree\\.bundle\\.js.*?\\],function\\(\\)\\{(.*?)\\}\\);',\n",
    "        full_text,\n",
    "        re.DOTALL\n",
    "    )\n",
    "    return blocks\n",
    "\n",
    "def extract_parent_doc_id(js_text: str):\n",
    "    match = re.search(r'\"obj_comp_id\"\\s*:\\s*\"(\\d+)\"', js_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def final_function(soup):\n",
    "    info_table = extract_named_tables(soup)\n",
    "    sections = parse_sections_with_subsections(soup)\n",
    "    info_sections = flatten_permit_data(sections)\n",
    "    blocks = extract_fancytree_blocks(soup)\n",
    "    info_tep = {}\n",
    "    for i, block in enumerate(blocks):\n",
    "        info_tep[f\"block_{i}\"] = extract_parent_doc_id(block)\n",
    "\n",
    "    # –ó'—î–¥–Ω—É—î–º–æ —Ç–∞–±–ª–∏—Ü—ñ info_table —Ç–∞ info_sections –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—ñ\n",
    "    base_df = pd.concat([info_table, info_sections], axis=1)\n",
    "    # –í–∏—Ç—è–≥—É—î–º–æ –≤—Å—ñ obj_comp_id —É —Å–ø–∏—Å–æ–∫\n",
    "    tep_ids = list(filter(None, info_tep.values()))\n",
    "    base_df[\"teps\"] = [tep_ids]\n",
    "    return base_df\n",
    "\n",
    "def fetch_page(doc_id, session, headers):\n",
    "    url = BASE_URL.format(doc_id)\n",
    "    headers[\"Referer\"] = url\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return doc_id, response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Issue with doc_id {doc_id}: {e}\")\n",
    "        return doc_id, None\n",
    "\n",
    "def parse_one(doc_id, session, headers):\n",
    "    doc_id, html = fetch_page(doc_id, session, headers)\n",
    "    if html is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        df = final_function(soup)\n",
    "\n",
    "        # Ensure result is DataFrame\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"‚ö†Ô∏è final_function did not return a DataFrame for {doc_id}\")\n",
    "            return None\n",
    "\n",
    "        df[\"doc_id\"] = doc_id\n",
    "        return df.to_dict(orient=\"records\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing doc_id {doc_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_permit_documents_parallel(ids_path: str, output_folder=\"parsed_chunks\", max_workers=10, chunk_size=3000):\n",
    "    # 1. Read doc_ids\n",
    "    df_ids = pd.read_csv(ids_path, sep=';')\n",
    "    doc_ids = df_ids['doc_id'].dropna().astype(str).tolist()\n",
    "\n",
    "    # 2. Prepare session & headers\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml\",\n",
    "    }\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    all_records = []\n",
    "    chunk_idx = 1\n",
    "\n",
    "    # 3. Multithreaded processing\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(parse_one, doc_id, session, headers): doc_id\n",
    "            for doc_id in doc_ids\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parsing documents\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_records.extend(result)\n",
    "\n",
    "                # Check if we reached chunk size\n",
    "                if len(all_records) >= chunk_size:\n",
    "                    df_chunk = pd.DataFrame(all_records)\n",
    "                    filename = os.path.join(output_folder, f\"permit_chunk_{chunk_idx:04}.csv\")\n",
    "                    df_chunk.to_csv(filename, sep=';', index=False)\n",
    "                    print(f\"üíæ Saved {len(df_chunk)} rows to {filename}\")\n",
    "                    all_records = []\n",
    "                    chunk_idx += 1\n",
    "\n",
    "    # Save remaining rows\n",
    "    if all_records:\n",
    "        df_chunk = pd.DataFrame(all_records)\n",
    "        filename = os.path.join(output_folder, f\"permit_chunk_{chunk_idx:04}.csv\")\n",
    "        df_chunk.to_csv(filename, sep=';', index=False)\n",
    "        print(f\"üíæ Saved last {len(df_chunk)} rows to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71adf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_permit_documents_parallel(\"permit_ids_final.csv\", output_folder=\"parsed_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d126b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39767/132843\n",
      "29.935337202562422\n"
     ]
    }
   ],
   "source": [
    "money_number = 0\n",
    "all_number = 0\n",
    "for n in range(1, 46):\n",
    "    try:\n",
    "        file_path = f'parsed_chunks/permit_chunk_{n:04}.csv'  # formats n with 4 digits, e.g. 0001\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        money_number += df[~df['–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞'].isna()].shape[0]\n",
    "        all_number += df.shape[0]\n",
    "    except FileNotFoundError:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"{money_number}/{all_number}\")\n",
    "print(money_number / all_number * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41ddeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parsed_chunks(input_folder=\"parsed_chunks\", output_file=\"permit_page_merged.csv\"):\n",
    "    all_files = sorted(glob(os.path.join(input_folder, \"*.csv\")))\n",
    "    all_dataframes = []\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=';')\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {file}: {e}\")\n",
    "\n",
    "    if all_dataframes:\n",
    "        merged_df = pd.concat(all_dataframes, axis=0, ignore_index=True, join='outer')\n",
    "        merged_df.to_csv(output_file, sep=';', index=False)\n",
    "        print(f\"‚úÖ Merged {len(all_dataframes)} chunks into {output_file} with {merged_df.shape[0]} rows and {merged_df.shape[1]} columns.\")\n",
    "    else:\n",
    "        print(\"‚ùå No CSV files were loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff95fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 45 chunks into permit_page_merged.csv with 132843 rows and 91 columns.\n"
     ]
    }
   ],
   "source": [
    "merge_parsed_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c1f0d",
   "metadata": {},
   "source": [
    "### Selecting Data for TEP Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95aff969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimit\\AppData\\Local\\Temp\\ipykernel_13148\\1853856987.py:1: DtypeWarning: Columns (90) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_df = pd.read_csv(\"permit_page_merged.csv\", sep=';')\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(\"permit_page_merged.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67fe0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\n",
      "–ó–∞ –ø—Ä–æ–µ–∫—Ç–æ–º, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\n",
      "–ù–∞ –±—É–¥. —Ä–æ–±–æ—Ç–∏, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\n",
      "–ù–∞ –º–∞—à–∏–Ω–∏, –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è —Ç–∞ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\n"
     ]
    }
   ],
   "source": [
    "for col in raw_df.columns:\n",
    "    if str(col).find('–≥—Ä–Ω') != -1:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f728c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ —Ö–æ—á–∞ –± –æ–¥–Ω–∏–º –∑–∞–ø–æ–≤–Ω–µ–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º: 47609\n",
      "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ —É—Å—ñ–º–∞ –∑–∞–ø–æ–≤–Ω–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏: 5220\n",
      "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤, –¥–µ '–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞' –∑–∞–ø–æ–≤–Ω–µ–Ω–∞: 39767\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\n",
    "    \"–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ó–∞ –ø—Ä–æ–µ–∫—Ç–æ–º, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ù–∞ –±—É–¥. —Ä–æ–±–æ—Ç–∏, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ù–∞ –º–∞—à–∏–Ω–∏, –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è —Ç–∞ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\"\n",
    "]\n",
    "\n",
    "at_least_one_notna = raw_df[columns_to_check].notna().any(axis=1).sum()\n",
    "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ —Ö–æ—á–∞ –± –æ–¥–Ω–∏–º –∑–∞–ø–æ–≤–Ω–µ–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º: {at_least_one_notna}\")\n",
    "\n",
    "all_notna = raw_df[columns_to_check].notna().all(axis=1).sum()\n",
    "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ —É—Å—ñ–º–∞ –∑–∞–ø–æ–≤–Ω–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏: {all_notna}\")\n",
    "\n",
    "count_general_filled = raw_df[\"–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\"].notna().sum()\n",
    "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤, –¥–µ '–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞' –∑–∞–ø–æ–≤–Ω–µ–Ω–∞: {count_general_filled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ac24b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    \"–ó–∞–≥–∞–ª—å–Ω–∞, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ó–∞ –ø—Ä–æ–µ–∫—Ç–æ–º, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ù–∞ –±—É–¥. —Ä–æ–±–æ—Ç–∏, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "    \"–ù–∞ –º–∞—à–∏–Ω–∏, –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è —Ç–∞ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä, —Ç–∏—Å. –≥—Ä–Ω._–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\"\n",
    "]\n",
    "\n",
    "df_filtered = raw_df[raw_df[columns_to_check].notna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f4ad5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(42717)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_filtered['teps'] != '[]').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f2794b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2            ['3475715044468066077', '3505494598975751463']\n",
       "7            ['2895385515312285052', '3264099843236169450']\n",
       "113          ['3409696146606524344', '3435001875726861471']\n",
       "127          ['3399028885969437987', '3444595903778784609']\n",
       "129          ['3219814599779943425', '3272336058095765008']\n",
       "                                ...                        \n",
       "132770    ['3418333938374935845', '102752710373728916819...\n",
       "132780       ['3217550036523025927', '3287560134212977959']\n",
       "132802    ['3474326641201645170', '548809842594968369583...\n",
       "132826       ['3475749604224403369', '3475760174952612936']\n",
       "132827       ['3511189562065945875', '3506954583450585008']\n",
       "Name: teps, Length: 7015, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[df_filtered['teps'].str.contains(',')]['teps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b60c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "teps\n",
       "0    40594\n",
       "1     7015\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_filtered['teps'].str.count(',')).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "475560fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teps</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['3475715044468066077', '3505494598975751463']</td>\n",
       "      <td>3509012527759492713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['2895385515312285052', '3264099843236169450']</td>\n",
       "      <td>3272199483311523749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>['3409696146606524344', '3435001875726861471']</td>\n",
       "      <td>3468346468056695877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>['3399028885969437987', '3444595903778784609']</td>\n",
       "      <td>3455313825610335322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>['3219814599779943425', '3272336058095765008']</td>\n",
       "      <td>3277211245282853984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132770</th>\n",
       "      <td>['3418333938374935845', '102752710373728916819...</td>\n",
       "      <td>3510615403648779855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132780</th>\n",
       "      <td>['3217550036523025927', '3287560134212977959']</td>\n",
       "      <td>3290794653204350870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132802</th>\n",
       "      <td>['3474326641201645170', '548809842594968369583...</td>\n",
       "      <td>3504733889669104912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132826</th>\n",
       "      <td>['3475749604224403369', '3475760174952612936']</td>\n",
       "      <td>3475591300688905491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132827</th>\n",
       "      <td>['3511189562065945875', '3506954583450585008']</td>\n",
       "      <td>3525635775364334867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7015 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     teps               doc_id\n",
       "2          ['3475715044468066077', '3505494598975751463']  3509012527759492713\n",
       "7          ['2895385515312285052', '3264099843236169450']  3272199483311523749\n",
       "113        ['3409696146606524344', '3435001875726861471']  3468346468056695877\n",
       "127        ['3399028885969437987', '3444595903778784609']  3455313825610335322\n",
       "129        ['3219814599779943425', '3272336058095765008']  3277211245282853984\n",
       "...                                                   ...                  ...\n",
       "132770  ['3418333938374935845', '102752710373728916819...  3510615403648779855\n",
       "132780     ['3217550036523025927', '3287560134212977959']  3290794653204350870\n",
       "132802  ['3474326641201645170', '548809842594968369583...  3504733889669104912\n",
       "132826     ['3475749604224403369', '3475760174952612936']  3475591300688905491\n",
       "132827     ['3511189562065945875', '3506954583450585008']  3525635775364334867\n",
       "\n",
       "[7015 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[df_filtered['teps'].str.count(',') > 0][['teps', 'doc_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3793ebd",
   "metadata": {},
   "source": [
    "–ø–æ–¥–∏–≤–∏–≤—Å—è –Ω–∞ –∫—ñ–ª—å–∫–∞ –¥–æ–∫—ñ–º–µ–Ω—Ç—ñ–≤, —â–æ –º–∞—é—Ç—å –∫—ñ–ª—å–∫–∞ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –¢–ï–ü, –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –∫—ñ–ª—å–∫–∞ —Ä–æ–∑–¥—ñ–ª—ñ–≤ –û–±'—î–∫—Ç–∏ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞.\n",
    "\n",
    "–¢—Ä–µ–±–∞ –±—Ä–∞—Ç–∏ –ü–µ—Ä—à–∏–π, –≤—ñ–Ω –Ω–∞–π–ø–æ–≤–Ω—ñ—à–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2486b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_tep(x):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed[0]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "df_filtered.loc[:, 'first_tep'] = df_filtered['teps'].apply(extract_first_tep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd85207",
   "metadata": {},
   "source": [
    "### Downloading TEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "364e4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_to_wide(df, doc_id, include_prim=True):\n",
    "    \"\"\"\n",
    "    –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î long DataFrame (–†–æ–∑–¥—ñ–ª, –ù–∞–∑–≤–∞, –ó–Ω–∞—á–µ–Ω–Ω—è, –ü—Ä–∏–º—ñ—Ç–∫–∞) —É wide DataFrame –∑ –æ–¥–Ω–∏–º doc_id.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Long —Ñ–æ—Ä–º–∞—Ç –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏ '–ù–∞–∑–≤–∞', '–ó–Ω–∞—á–µ–Ω–Ω—è', '–ü—Ä–∏–º—ñ—Ç–∫–∞'\n",
    "        doc_id (str): –ó–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è doc_id, —è–∫–µ –±—É–¥–µ –¥–æ–¥–∞–Ω–µ –¥–æ –≤—Å—ñ—Ö —Ä—è–¥–∫—ñ–≤\n",
    "        include_prim (bool): –ß–∏ –≤–∫–ª—é—á–∞—Ç–∏ '–ü—Ä–∏–º—ñ—Ç–∫–∞' —è–∫ –æ–∫—Ä–µ–º—ñ –∫–æ–ª–æ–Ω–∫–∏\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: –û–¥–∏–Ω —Ä—è–¥–æ–∫ wide-—Ñ–æ—Ä–º–∞—Ç—É –∑ doc_id\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"doc_id\"] = doc_id\n",
    "\n",
    "    wide_df = df.pivot_table(index=\"doc_id\", columns=\"–ù–∞–∑–≤–∞\", values=\"–ó–Ω–∞—á–µ–Ω–Ω—è\", aggfunc=\"first\")\n",
    "\n",
    "    if include_prim and \"–ü—Ä–∏–º—ñ—Ç–∫–∞\" in df.columns:\n",
    "        prim_df = df.pivot_table(index=\"doc_id\", columns=\"–ù–∞–∑–≤–∞\", values=\"–ü—Ä–∏–º—ñ—Ç–∫–∞\", aggfunc=\"first\")\n",
    "        prim_df.columns = [f\"{col} (–ø—Ä–∏–º—ñ—Ç–∫–∞)\" for col in prim_df.columns]\n",
    "        wide_df = pd.concat([wide_df, prim_df], axis=1)\n",
    "\n",
    "    wide_df = wide_df.reset_index()\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbf983ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL_TEP = \"https://e-construction.gov.ua/document_detail_tep/doc_id={}\"\n",
    "\n",
    "def fetch_html(tep_doc_id, session, headers):\n",
    "    url = BASE_URL_TEP.format(tep_doc_id)\n",
    "    headers[\"Referer\"] = url\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return tep_doc_id, response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error for {tep_doc_id}: {e}\")\n",
    "        return tep_doc_id, None\n",
    "\n",
    "def parse_one_tep(tep_doc_id, session, headers):\n",
    "    tep_doc_id, html = fetch_html(tep_doc_id, session, headers)\n",
    "    if html is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        long_df = parse_tep_info(soup)\n",
    "        wide_df = long_to_wide(long_df, tep_doc_id)\n",
    "        return wide_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed parsing tep_doc_id {tep_doc_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_last_chunk_index(output_folder):\n",
    "    existing_files = os.listdir(output_folder)\n",
    "    chunk_nums = [int(re.search(r'(\\d+)', f).group(1)) for f in existing_files if f.startswith(\"tep_chunk_\") and f.endswith(\".csv\")]\n",
    "    return max(chunk_nums, default=0)\n",
    "\n",
    "def process_all_teps(df_with_teps, output_folder=\"tep_chunks\", max_workers=10, chunk_size=500):\n",
    "    tep_ids = df_with_teps[\"first_tep\"].dropna().astype(str).unique().tolist()\n",
    "    \n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml\",\n",
    "    }\n",
    "\n",
    "    # 2. –û—Ç—Ä–∏–º–∞—î–º–æ doc_id –∑ tep_merged.csv (–≤–∂–µ —Å–ø–∞—Ä—Å–µ–Ω—ñ ‚Äî —Ç—Ä–µ–±–∞ –≤–∏–∫–ª—é—á–∏—Ç–∏)\n",
    "    try:\n",
    "        df_existing = pd.read_csv(\"tep_merged.csv\", sep=';')\n",
    "        parsed_doc_ids = df_existing[\"doc_id\"].dropna().astype(str).unique().tolist()\n",
    "        print(f\"üì¶ Loaded {len(parsed_doc_ids)} already parsed doc_ids from tep_merged.csv\")\n",
    "    except FileNotFoundError:\n",
    "        parsed_doc_ids = []\n",
    "        print(\"‚ö†Ô∏è File 'tep_merged.csv' not found. Proceeding with all TEPs.\")\n",
    "\n",
    "    # 3. –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ, —â–æ —â–µ –Ω–µ –±—É–ª–∏ –ø–∞—Ä—Å–µ–Ω—ñ\n",
    "    tep_ids_to_parse = [tep_id for tep_id in tep_ids if tep_id not in parsed_doc_ids]\n",
    "    print(f\"üîç Will parse {len(tep_ids_to_parse)} new TEPs\")\n",
    "\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    chunk_idx = get_last_chunk_index(output_folder) + 1\n",
    "    current_chunk = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(parse_one_tep, tep_id, session, headers): tep_id\n",
    "            for tep_id in tep_ids_to_parse\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parsing TEPs\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                current_chunk.append(result)\n",
    "\n",
    "                if len(current_chunk) >= chunk_size:\n",
    "                    df_chunk = pd.concat(current_chunk, ignore_index=True)\n",
    "                    filename = os.path.join(output_folder, f\"tep_chunk_{chunk_idx:04}.csv\")\n",
    "                    df_chunk.to_csv(filename, sep=';', index=False)\n",
    "                    print(f\"üíæ Saved {len(df_chunk)} rows to {filename}\")\n",
    "                    current_chunk = []\n",
    "                    chunk_idx += 1\n",
    "\n",
    "    # Save any remaining\n",
    "    if current_chunk:\n",
    "        df_chunk = pd.concat(current_chunk, ignore_index=True)\n",
    "        filename = os.path.join(output_folder, f\"tep_chunk_{chunk_idx:04}.csv\")\n",
    "        df_chunk.to_csv(filename, sep=';', index=False)\n",
    "        print(f\"üíæ Saved last {len(df_chunk)} rows to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dc573f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimit\\AppData\\Local\\Temp\\ipykernel_13148\\2732572972.py:44: DtypeWarning: Columns (0,2,4,14,15,16,31,32,33,35,37,38,39,40,41,42,48,49,50,55,56,57,58,59,63,69,72,74,75,77,78,80,81,82,84,85,86,88,90,91,93,96,98,99,101,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,172,173,174,175,177,178,180,182,183,184,185,186,187,188,193,194,195,196,198,200,202,205,207,208,209,210,212,214,215,216,217,219,220,223,224,227,228,233,234,235,236,241,242,243,244,245,246,248,250,253,254,255,256,260,261,262,264,276,277,278,279,280,281,282,283,284,285,286,288,293,294,295,296,300,301,302,303,308,309,310,311,312,316,317,318,321,324,327,328,329,330,332,334,336,340,341,342,346,347,348,351,355,358,359,361,362,365,366,368,370,372,374,384,389,390,391,392,414,415,416,417,419,420,424,426,427,428,430,432,434,437,438,441,442,443,444,446,448,450,454,455,456,459,460,462,464,466,468,469,470,476,478,481,482,486,488,490,493,494,499,500,502,521,527,528,536,542,544,547,550,554,555,556,558,562,564,566,568,572,578,580,582,586,588,590,592,594,600,602,605,608,612,614,638,640,648,652,658,660,670,674,676,678,682,694) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_existing = pd.read_csv(\"tep_merged.csv\", sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loaded 35834 already parsed doc_ids from tep_merged.csv\n",
      "üîç Will parse 1 new TEPs\n",
      "üîç Will parse 1 new TEPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing TEPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved last 1 rows to tep_chunks\\tep_chunk_0074.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_all_teps(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dcb059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 74 chunks into tep_merged.csv with 35835 rows.\n"
     ]
    }
   ],
   "source": [
    "def merge_tep_chunks(input_folder=\"tep_chunks\", output_file=\"tep_merged.csv\"):\n",
    "    all_files = sorted(glob(os.path.join(input_folder, \"*.csv\")))\n",
    "    if not all_files:\n",
    "        print(\"‚ö†Ô∏è No CSV files found.\")\n",
    "        return\n",
    "\n",
    "    dataframes = []\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=';')\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {file}: {e}\")\n",
    "\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "        merged_df.to_csv(output_file, sep=';', index=False)\n",
    "        print(f\"‚úÖ Merged {len(all_files)} chunks into {output_file} with {merged_df.shape[0]} rows.\")\n",
    "    else:\n",
    "        print(\"‚ùå No data loaded from chunk files.\")\n",
    "\n",
    "merge_tep_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
