{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser for Expertise Data from the EDECCB Registry\n",
    "\n",
    "This script automates the process of collecting information about conducted expertise of construction documentation from the open EDECCB registry (https://e-construction.gov.ua/document/optype=6).\n",
    "\n",
    "### Main steps of the script:\n",
    "1. Downloading HTML pages of expertise using `requests` and `BeautifulSoup` libraries.\n",
    "2. Extracting key data from each expertise page:\n",
    "   - Expertise name\n",
    "   - Organization issuing the expertise\n",
    "   - Object consequence class\n",
    "   - Type of construction\n",
    "   - Additional information (estimate, design stage, etc.)\n",
    "3. Processing a large number of pages using pagination.\n",
    "4. Creating a structured dataset (`pandas.DataFrame`) for further analysis.\n",
    "5. Saving the obtained data to a file for future use.\n",
    "\n",
    "### Libraries used:\n",
    "- `requests`\n",
    "- `beautifulsoup4`\n",
    "- `pandas`\n",
    "- `time`\n",
    "- `re`\n",
    "- `os`\n",
    "\n",
    "### Notes:\n",
    "- Implemented retries in case of temporary connection failures.\n",
    "- The script extracts complete information for further anomaly detection in the expertise registry.\n",
    "- Data may contain minor gaps due to site structure limitations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for HTTP requests, HTML parsing, and data processing\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining HTTP headers to mimic a browser and avoid being blocked by the server\n",
    "headers = {\n",
    "        \"User-Agent\": \"Samsung/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Language\": \"uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7,de-DE;q=0.6,de;q=0.5\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Registry\n",
    "This section defines the base URL and functions for parsing the registry pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the expertise registry\n",
    "BASE_URL = \"https://e-construction.gov.ua/document/optype=6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract document IDs and names from an HTML page\n",
    "def extract_doc_ids_and_names(soup, page_num=0):\n",
    "    \"\"\"\n",
    "    Extract document IDs and names from the HTML content of a page.\n",
    "\n",
    "    This function parses the HTML content of a page to find document IDs and their corresponding names.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the page.\n",
    "        page_num (int): The page number being processed (default is 0).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            - doc_id (str): The document ID.\n",
    "            - name (str): The document name.\n",
    "            - page_num (int): The page number.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Find all entries containing doc_id and name\n",
    "    for item in soup.find_all(\"div\", class_=\"dataset__item\"):\n",
    "        # Extract doc_id from <a> links\n",
    "        link = item.find(\"a\", class_=\"opendata__link\")\n",
    "        if link and \"href\" in link.attrs:\n",
    "            href = link[\"href\"]\n",
    "            if \"doc_id=\" in href:\n",
    "                doc_id = href.split(\"doc_id=\")[1].split(\"/\")[0]  # Extract only the ID\n",
    "            else:\n",
    "                doc_id = None\n",
    "        else:\n",
    "            doc_id = None\n",
    "\n",
    "        # Extract text from <h3 class=\"opendata__name\">\n",
    "        name_tag = item.find(\"h3\", class_=\"opendata__name\")\n",
    "        name = name_tag.text.strip() if name_tag else None\n",
    "\n",
    "        # Add to the list if both ID and name are present\n",
    "        if doc_id and name:\n",
    "            results.append((doc_id, name, page_num))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to fetch the HTML content of a single page\n",
    "def fetch_page(page, session, headers):\n",
    "    \"\"\"\n",
    "    Fetch the HTML content of a single page.\n",
    "\n",
    "    This function sends an HTTP GET request to fetch the HTML content of a specific page.\n",
    "\n",
    "    Args:\n",
    "        page (int): The page number to fetch.\n",
    "        session (requests.Session): The session object for making HTTP requests.\n",
    "        headers (dict): HTTP headers to include in the request.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - page (int): The page number.\n",
    "            - html (str or None): The HTML content of the page, or None if the request fails.\n",
    "    \"\"\"\n",
    "    url = BASE_URL if page == 1 else BASE_URL + f\"/page={page}\"\n",
    "    headers[\"Referer\"] = url\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return page, response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Problem with page {page}: {e}\")\n",
    "        return page, None  # Return None if the request fails\n",
    "\n",
    "# Function to scrape multiple pages and save the results to a CSV file\n",
    "def scrape_pages(start_page=1, end_page=5579, save_interval=50, output_file=\"documents.csv\", max_workers=10):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages for document data and save the results to a CSV file.\n",
    "\n",
    "    This function iterates through a range of pages, extracts document data, and saves the results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        start_page (int): The starting page number (default is 1).\n",
    "        end_page (int): The ending page number (default is 5579).\n",
    "        save_interval (int): The number of pages to process before saving data to the file (default is 50).\n",
    "        output_file (str): The name of the output CSV file (default is 'documents.csv').\n",
    "        max_workers (int): The maximum number of threads to use for parallel processing (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of page numbers that could not be processed.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    skipped_pages = []\n",
    "    column_names = [\"doc_id\", \"name\", \"page_num\"]\n",
    "\n",
    "    session = requests.Session()  # Use a session for faster requests\n",
    "    pages_parsed = 0  # Counter for processed pages\n",
    "\n",
    "    # Read the existing file to avoid duplicating work\n",
    "    try:\n",
    "        existing_data = pd.read_csv(output_file)\n",
    "        last_page = existing_data[\"page_num\"].max()\n",
    "        start_page = last_page + 1 if last_page > start_page else start_page\n",
    "        print(f\"‚ö° Resuming from page {start_page}\")\n",
    "    except (IndexError, FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(\"üîπ Starting from scratch!\")\n",
    "        pd.DataFrame(columns=column_names).to_csv(output_file, index=False)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_page = {executor.submit(fetch_page, page, session, headers): page for page in range(start_page, end_page + 1)}\n",
    "\n",
    "        for future in as_completed(future_to_page):\n",
    "            page = future_to_page[future]\n",
    "            try:\n",
    "                page, html = future.result()\n",
    "                if html is None:\n",
    "                    skipped_pages.append(page)\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                data = extract_doc_ids_and_names(soup, page_num=page)\n",
    "                all_data.extend(data)\n",
    "                pages_parsed += 1  # Increment the counter for processed pages\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing page {page}: {e}\")\n",
    "                skipped_pages.append(page)\n",
    "\n",
    "            # Save data every `save_interval` pages\n",
    "            if len(all_data) >= save_interval * 12:\n",
    "                df = pd.DataFrame(all_data, columns=column_names)\n",
    "                df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "                print(f\"üìÑ Saved pages: {pages_parsed}/{end_page - start_page + 1}\")\n",
    "                all_data = []  # Clear the list after saving\n",
    "\n",
    "    # Final save if there is remaining data\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data, columns=column_names)\n",
    "        df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "        print(f\"‚úÖ Final save of {len(all_data)} records.\")\n",
    "        all_data = []\n",
    "\n",
    "    print(f\"üîö Completed! Skipped pages: {len(skipped_pages)}\")\n",
    "\n",
    "    # Retry skipped pages\n",
    "    still_skipped_pages = []\n",
    "    if skipped_pages:\n",
    "        print(\"üîÑ Retrying failed pages...\")\n",
    "        for sk_page in skipped_pages:\n",
    "            page, html = fetch_page(sk_page, session, headers)\n",
    "            if html is None:\n",
    "                print(f\"‚ùå Failed to retry page {sk_page}\")\n",
    "                still_skipped_pages.append(sk_page)\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            data = extract_doc_ids_and_names(soup, page_num=sk_page)\n",
    "            all_data.extend(data)\n",
    "            df = pd.DataFrame(all_data, columns=column_names)\n",
    "            df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "            print(f\"üìÑ Retried page {sk_page}\")\n",
    "            all_data = []\n",
    "        print(\"üîö Retry completed!\")\n",
    "    print(f\"üîö Final skipped pages: {len(still_skipped_pages)}\")\n",
    "    return still_skipped_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of calling the scrape_pages function to parse and save data\n",
    "# still_skipped_pages = scrape_pages(output_file='documents.csv', max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()  # Use a session for faster requests\n",
    "# column_names = [\"doc_id\", \"name\", \"page_num\"]\n",
    "# output_file = \"documents.csv\"\n",
    "# all_data = []\n",
    "\n",
    "# for sk_page in still_skipped_pages:\n",
    "#     page, html = fetch_page(sk_page, session, headers)\n",
    "#     if html is None:\n",
    "#         print(f\"‚ùå Failed to retry page {sk_page}\")\n",
    "#         continue\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "#     data = extract_doc_ids_and_names(soup, page_num=sk_page)\n",
    "#     all_data.extend(data)\n",
    "#     df = pd.DataFrame(all_data, columns=column_names)\n",
    "#     df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "#     print(f\"üìÑ Retried page {sk_page}\")\n",
    "#     all_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT PAGE\n",
    "This section defines the document IDs and fetches the HTML content of a specific document page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document IDs for different cases\n",
    "doc_id_normal = \"3524292411700282436\" # Normal doc_id\n",
    "doc_id_empty = \"3505533101101024560\" # Empty doc_id\n",
    "doc_id_invalid = \"3524292411700000000\" # Invalid doc_id\n",
    "doc_id_double = \"3454546322529454021\" # Double doc_id\n",
    "url = f\"https://e-construction.gov.ua/document_detail/doc_id={doc_id_normal}/optype=6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the HTML content of the page\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parsing the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all column names from the CSV file\n",
    "all_columns = pd.read_csv(\"column_name_mapping.csv\", delimiter=\";\")['ukrainian_name'].tolist()\n",
    "\n",
    "# Function to clean text by removing unnecessary characters\n",
    "def clean_text(text):\n",
    "    return text.strip().replace(\";\", \",\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "# Function to extract expertise data from the HTML content\n",
    "def extract_expertise_data(soup, all_columns):\n",
    "    \"\"\"\n",
    "    Extract expertise data from the HTML content.\n",
    "\n",
    "    This function parses the HTML content of a document page to extract information about expertise, cost estimates, and the customer.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML content.\n",
    "        all_columns (list): A list of all column names for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted data, with columns matching `all_columns`.\n",
    "    \"\"\"\n",
    "    main_info = {}\n",
    "    key_counts = {\"–û—Ä–≥–∞–Ω, —â–æ –≤–∏–¥–∞–≤\": 0}\n",
    "\n",
    "    # --- 1. Expertise Data ---\n",
    "    for item in soup.find_all(\"div\", class_=\"object-info-item\"):\n",
    "        key = clean_text(item.find(\"div\", class_=\"object-info_left\").text)\n",
    "        value = clean_text(item.find(\"div\", class_=\"object-info_right\").text)\n",
    "        if key in key_counts:\n",
    "            key_counts[key] += 1\n",
    "            if key_counts[key] == 1:\n",
    "                key += \"_–µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∞\"\n",
    "            elif key_counts[key] == 2:\n",
    "                key += \"_–ü–ö–î\"\n",
    "        main_info[key] = value\n",
    "\n",
    "    # --- 2. Cost Estimate Data ---\n",
    "\n",
    "\n",
    "    costs_header = soup.find(\"h3\", id=\"deviations_building_codes_psm_1\")\n",
    "    if costs_header:\n",
    "        wrapper_div = costs_header.find_next_sibling(\"div\")\n",
    "\n",
    "        if wrapper_div:\n",
    "            if wrapper_div.find(\"span\", class_=\"object-no-info\"):\n",
    "                main_info[\"–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è\"] = \"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –∑–∞–∑–Ω–∞—á–µ–Ω–æ\"\n",
    "\n",
    "            elif wrapper_div.find(\"table\"):\n",
    "                table = wrapper_div.find(\"table\")\n",
    "                tbody = table.find(\"tbody\")\n",
    "                first_row = tbody.find(\"tr\") if tbody else None\n",
    "\n",
    "                if first_row:\n",
    "                    cells = first_row.find_all(\"td\")\n",
    "                    if len(cells) >= 3:\n",
    "                        raw_code = clean_text(cells[0].text)\n",
    "                        date = clean_text(cells[1].text)\n",
    "                        cost = clean_text(cells[2].text)\n",
    "\n",
    "                        match = re.match(r\"(.+?)\\s+–†–µ–¥–∞–∫—Ü—ñ—è\\s+(‚Ññ\\d+)\", raw_code)\n",
    "                        code, redaction = match.groups() if match else (raw_code, \"\")\n",
    "\n",
    "                        main_info[\"–ö–æ–¥ –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = code\n",
    "                        main_info[\"–ù–æ–º–µ—Ä —Ä–µ–¥–∞–∫—Ü—ñ—ó –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = redaction\n",
    "                        main_info[\"–î–∞—Ç–∞ –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = date\n",
    "                        main_info[\"–ó–∞—è–≤–ª–µ–Ω–∞ –∫–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å, —Ç–∏—Å. –≥—Ä–Ω.\"] = cost\n",
    "\n",
    "    # --- 3. Customer Data ---\n",
    "    section = soup.find(\"h3\", id=\"doc_applicants\")\n",
    "    if section:\n",
    "        table = section.find_next(\"table\", class_=\"object-table\")\n",
    "        if table:\n",
    "            tbody = table.find(\"tbody\")\n",
    "            if tbody:\n",
    "                first_row = tbody.find(\"tr\")\n",
    "                if first_row:\n",
    "                    cells = first_row.find_all(\"td\")\n",
    "                    if len(cells) >= 2:\n",
    "                        legal_status = clean_text(cells[0].text)\n",
    "                        raw_name = clean_text(cells[1].text)\n",
    "                        match = re.search(r\"\\((\\–¥{8})\\)\", raw_name)\n",
    "                        edrpou = match.group(1) if match else \"\"\n",
    "                        name = re.sub(r\"\\—Å*\\(\\–¥{8}\\)\\—Å*$\", \"\", raw_name).strip()\n",
    "                        main_info[\"–ü—Ä–∞–≤–æ–≤–∏–π —Å—Ç–∞—Ç—É—Å –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = legal_status\n",
    "                        main_info[\"–ù–∞–∑–≤–∞ –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = name\n",
    "                        main_info[\"–Ñ–î–†–ü–û–£ –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = edrpou\n",
    "\n",
    "    # --- Create Final DataFrame ---\n",
    "    df = pd.DataFrame([{col: main_info.get(col, \"\") for col in all_columns}])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_versions(soup, doc_id):\n",
    "    \"\"\"\n",
    "    Extracts all document versions (doc_id and date) from the 'doc-versions' section of the HTML.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the page.\n",
    "        doc_id (str): The document ID of the main document.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are document IDs and values are dictionaries containing\n",
    "              the version and date of each document.\n",
    "    \"\"\"\n",
    "    versions = {}\n",
    "    # Locate the section containing document versions\n",
    "    versions_section = soup.find(\"div\", class_=\"doc-versions\")\n",
    "    if versions_section:\n",
    "        # Iterate through all links in the 'doc-versions' section\n",
    "        for link in versions_section.find_all(\"a\", class_=\"doc-versions__item\"):\n",
    "            if \"—Ä–µ–¥–∞–∫—Ü—ñ—è\" in link.text:  # Check if the link text contains \"—Ä–µ–¥–∞–∫—Ü—ñ—è\" (version)\n",
    "                match = re.search(r'doc_id=(\\d+)', link[\"href\"])  # Extract doc_id from the link\n",
    "                date_match = re.search(r'(\\d{2}\\.\\d{2}\\.\\d{4})', link.text)  # Extract date from the link text\n",
    "                if match and date_match:\n",
    "                    versions[match.group(1)] = {\n",
    "                        \"date\": date_match.group(1)    # Date of the version\n",
    "                    }\n",
    "        \n",
    "        # Find the active version (currently selected version)\n",
    "        active_version = versions_section.find(\"span\", class_=\"doc-versions__item active\")\n",
    "        if active_version:\n",
    "            active_match = re.search(r'(\\d+) —Ä–µ–¥–∞–∫—Ü—ñ—è –≤—ñ–¥ (\\d{2}\\.\\d{2}\\.\\d{4})', active_version.text)\n",
    "            if active_match:\n",
    "                versions[doc_id] = {\n",
    "                    \"date\": active_match.group(2)            # Date of the active version\n",
    "                }\n",
    "    return versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3445958426151093499': {'date': '30.08.2024'},\n",
       " '3223037525790557843': {'date': '16.12.2024'}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of calling the extract_doc_versions function\n",
    "extract_doc_versions(soup, \"3223037525790557843\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final parsing!\n",
    "This section defines the functions for fetching and parsing document data, including handling document versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the document detail page\n",
    "BASE_URL = \"https://e-construction.gov.ua/document_detail/doc_id={}/optype=6\"\n",
    "# Read all column names from the CSV file\n",
    "all_columns = pd.read_csv(\"column_name_mapping.csv\", delimiter=\";\")['ukrainian_name'].tolist()\n",
    "\n",
    "# Function to fetch the HTML content of a document page using the provided doc_id\n",
    "def fetch_page(doc_id, session, headers):\n",
    "    \"\"\"\n",
    "    Fetch the HTML content of a document page using the provided doc_id.\n",
    "\n",
    "    Args:\n",
    "        doc_id (str): The document ID to fetch.\n",
    "        session (requests.Session): The session object for making HTTP requests.\n",
    "        headers (dict): The HTTP headers to include in the request.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the doc_id and the HTML content (or None if an error occurs).\n",
    "    \"\"\"\n",
    "    url = BASE_URL.format(doc_id)\n",
    "    headers[\"Referer\"] = url\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return doc_id, response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Issue with doc_id {doc_id}: {e}\")\n",
    "        return doc_id, None\n",
    "\n",
    "# Function to clean text by removing unnecessary characters\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing unnecessary characters and formatting.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    return text.strip().replace(\";\", \",\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "# Function to extract expertise data from the HTML content\n",
    "def extract_expertise_data(soup, all_columns):\n",
    "    \"\"\"\n",
    "    Extract expertise data from the HTML content.\n",
    "\n",
    "    This function parses the HTML content of a document page to extract information about expertise, cost estimates, and the customer.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML content.\n",
    "        all_columns (list): A list of all column names for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted data, with columns matching `all_columns`.\n",
    "    \"\"\"\n",
    "    main_info = {}\n",
    "    key_counts = {\"–û—Ä–≥–∞–Ω, —â–æ –≤–∏–¥–∞–≤\": 0}\n",
    "\n",
    "    # --- 1. Expertise Data ---\n",
    "    for item in soup.find_all(\"div\", class_=\"object-info-item\"):\n",
    "        key = clean_text(item.find(\"div\", class_=\"object-info_left\").text)\n",
    "        value = clean_text(item.find(\"div\", class_=\"object-info_right\").text)\n",
    "        if key in key_counts:\n",
    "            key_counts[key] += 1\n",
    "            if key_counts[key] == 1:\n",
    "                key += \"_–µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∞\"\n",
    "            elif key_counts[key] == 2:\n",
    "                key += \"_–ü–ö–î\"\n",
    "        main_info[key] = value\n",
    "\n",
    "    # --- 2. Cost Estimate Data ---\n",
    "\n",
    "\n",
    "    costs_header = soup.find(\"h3\", id=\"deviations_building_codes_psm_1\")\n",
    "    if costs_header:\n",
    "        wrapper_div = costs_header.find_next_sibling(\"div\")\n",
    "\n",
    "        if wrapper_div:\n",
    "            if wrapper_div.find(\"span\", class_=\"object-no-info\"):\n",
    "                main_info[\"–ö–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è\"] = \"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –∑–∞–∑–Ω–∞—á–µ–Ω–æ\"\n",
    "\n",
    "            elif wrapper_div.find(\"table\"):\n",
    "                table = wrapper_div.find(\"table\")\n",
    "                tbody = table.find(\"tbody\")\n",
    "                first_row = tbody.find(\"tr\") if tbody else None\n",
    "\n",
    "                if first_row:\n",
    "                    cells = first_row.find_all(\"td\")\n",
    "                    if len(cells) >= 3:\n",
    "                        raw_code = clean_text(cells[0].text)\n",
    "                        date = clean_text(cells[1].text)\n",
    "                        cost = clean_text(cells[2].text)\n",
    "\n",
    "                        match = re.match(r\"(.+?)\\—Å+–†–µ–¥–∞–∫—Ü—ñ—è\\—Å+(‚Ññ\\–¥+)\", raw_code)\n",
    "                        code, redaction = match.groups() if match else (raw_code, \"\")\n",
    "\n",
    "                        main_info[\"–ö–æ–¥ –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = code\n",
    "                        main_info[\"–ù–æ–º–µ—Ä —Ä–µ–¥–∞–∫—Ü—ñ—ó –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = redaction\n",
    "                        main_info[\"–î–∞—Ç–∞ –∫–æ—à—Ç–æ—Ä–∏—Å—É\"] = date\n",
    "                        main_info[\"–ó–∞—è–≤–ª–µ–Ω–∞ –∫–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å, —Ç–∏—Å. –≥—Ä–Ω.\"] = cost\n",
    "\n",
    "    # --- 3. Customer Data ---\n",
    "    section = soup.find(\"h3\", id=\"doc_applicants\")\n",
    "    if section:\n",
    "        table = section.find_next(\"table\", class_=\"object-table\")\n",
    "        if table:\n",
    "            tbody = table.find(\"tbody\")\n",
    "            if tbody:\n",
    "                first_row = tbody.find(\"tr\")\n",
    "                if first_row:\n",
    "                    cells = first_row.find_all(\"td\")\n",
    "                    if len(cells) >= 2:\n",
    "                        legal_status = clean_text(cells[0].text)\n",
    "                        raw_name = clean_text(cells[1].text)\n",
    "                        match = re.search(r\"\\((\\–¥{8})\\)\", raw_name)\n",
    "                        edrpou = match.group(1) if match else \"\"\n",
    "                        name = re.sub(r\"\\—Å*\\(\\–¥{8}\\)\\—Å*$\", \"\", raw_name).strip()\n",
    "                        main_info[\"–ü—Ä–∞–≤–æ–≤–∏–π —Å—Ç–∞—Ç—É—Å –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = legal_status\n",
    "                        main_info[\"–ù–∞–∑–≤–∞ –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = name\n",
    "                        main_info[\"–Ñ–î–†–ü–û–£ –∑–∞–º–æ–≤–Ω–∏–∫–∞\"] = edrpou\n",
    "\n",
    "    # --- Create Final DataFrame ---\n",
    "    df = pd.DataFrame([{col: main_info.get(col, \"\") for col in all_columns}])\n",
    "    return df\n",
    "\n",
    "# Function to extract all document versions from the HTML content\n",
    "def extract_doc_versions(soup, doc_id):\n",
    "    \"\"\"\n",
    "    Extract all document versions from the HTML content.\n",
    "\n",
    "    This function parses the 'doc-versions' section of the HTML content to extract all versions of a document, including their IDs and dates.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML content.\n",
    "        doc_id (str): The primary document ID.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are document IDs and values are dictionaries containing:\n",
    "            - 'date' (str): The date of the version.\n",
    "    \"\"\"\n",
    "    versions = {}\n",
    "    versions_section = soup.find(\"div\", class_=\"doc-versions\")\n",
    "    if versions_section:\n",
    "        for link in versions_section.find_all(\"a\", class_=\"doc-versions__item\"):\n",
    "            if \"—Ä–µ–¥–∞–∫—Ü—ñ—è\" in link.text:\n",
    "                match = re.search(r'doc_id=(\\–¥+)', link[\"href\"])\n",
    "                date_match = re.search(r'(\\–¥{2}\\.\\–¥{2}\\.\\–¥{4})', link.text)\n",
    "                if match and date_match:\n",
    "                    versions[match.group(1)] = {\"date\": date_match.group(1)}\n",
    "        \n",
    "        active_version = versions_section.find(\"span\", class_=\"doc-versions__item active\")\n",
    "        if active_version:\n",
    "            active_match = re.search(r'(\\–¥+) —Ä–µ–¥–∞–∫—Ü—ñ—è –≤—ñ–¥ (\\–¥{2}\\.\\–¥{2}\\.\\–¥{4})', active_version.text)\n",
    "            if active_match:\n",
    "                versions[doc_id] = {\"date\": active_match.group(2)}\n",
    "    return versions\n",
    "\n",
    "# Function to parse document data from a CSV file and save the results to an output file\n",
    "def parse_documents(csv_file, all_columns, output_file=\"parsed_documents.csv\", max_workers=10, save_interval=2000):\n",
    "    \"\"\"\n",
    "    Parse document expertise data from a CSV file and save the results to an output file.\n",
    "\n",
    "    This function reads document IDs from a CSV file, fetches their HTML content, extracts relevant data, and saves the results to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): The input CSV file containing document IDs.\n",
    "        output_file (str): The output CSV file to save the results.\n",
    "        max_workers (int): The maximum number of threads to use for parallel processing.\n",
    "        save_interval (int): The number of records to save at each interval.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Create the output file if it doesn't exist\n",
    "    if not os.path.exists(output_file):\n",
    "        pd.DataFrame(columns=all_columns).to_csv(output_file, index=False, sep=\";\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    doc_ids = df[\"doc_id\"].unique()\n",
    "\n",
    "    documents_parsed = 0\n",
    "    skipped_versions = []\n",
    "    skipped_documents = []\n",
    "    versions_cache = {}  # Cache for storing document versions\n",
    "    additional_futures = []\n",
    "    primary_docs_processed = 0  # Counter for primary documents\n",
    "    versions_to_process = 0  # Counter for versions to process\n",
    "    buffer = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_doc = {}\n",
    "        # Add all primary documents to the queue\n",
    "        for doc_id in doc_ids:\n",
    "            future_to_doc[executor.submit(fetch_page, doc_id, session, headers)] = doc_id\n",
    "        print(len(future_to_doc), \"primary documents in the queue\")\n",
    "\n",
    "        for future in as_completed(future_to_doc):\n",
    "            doc_id = future_to_doc[future]\n",
    "            try:\n",
    "                doc_id, html = future.result()\n",
    "                if html is None:\n",
    "                    skipped_documents.append(doc_id)\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                data_df_orig = extract_expertise_data(soup, all_columns)\n",
    "\n",
    "                # Use cache if available; otherwise, call extract_doc_versions\n",
    "                if doc_id not in versions_cache:\n",
    "                    versions_cache[doc_id] = extract_doc_versions(soup, doc_id)\n",
    "\n",
    "                versions = versions_cache[doc_id]\n",
    "\n",
    "                data_df_orig[\"doc_id\"] = doc_id\n",
    "                data_df_orig[\"date\"] = versions.get(doc_id, {}).get(\"date\", \"Unknown\")\n",
    "                data_df_orig[\"version\"] = versions.get(doc_id, {}).get(\"version\", \"Unknown\")\n",
    "\n",
    "                buffer.append(data_df_orig)\n",
    "                documents_parsed += 1\n",
    "                primary_docs_processed += 1  # Primary document processed\n",
    "\n",
    "                # Add document versions to the queue for parsing\n",
    "                for version_doc_id, version_data in versions.items():\n",
    "                    if version_doc_id not in versions_cache:  # Avoid duplicate parsing\n",
    "                        versions_cache[version_doc_id] = version_data\n",
    "                        additional_futures.append(executor.submit(fetch_page, version_doc_id, session, headers))\n",
    "                        future_to_doc[additional_futures[-1]] = version_doc_id\n",
    "                        versions_to_process += 1  # Count versions to process\n",
    "\n",
    "                if len(buffer) >= save_interval:\n",
    "                    result_df = pd.concat(buffer, ignore_index=True)\n",
    "                    result_df.to_csv(output_file, mode='a', index=False, header=False, sep=\";\")\n",
    "                    buffer = []\n",
    "                    print(f\"üìÑ Primary documents processed: {primary_docs_processed}/{len(doc_ids)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_documents.append(doc_id)\n",
    "                print(f\"‚ö†Ô∏è Error processing doc_id {doc_id}: {e}\")\n",
    "\n",
    "        if buffer:\n",
    "            result_df = pd.concat(buffer, ignore_index=True)\n",
    "            result_df.to_csv(output_file, mode='a', index=False, header=False, sep=\";\")\n",
    "            print(f\"‚úÖ Final save of {len(buffer)} records.\")\n",
    "            buffer = []\n",
    "        if skipped_documents:\n",
    "            skipped_df = pd.DataFrame(skipped_documents)\n",
    "            skipped_df.to_csv(\"skipped_documents.csv\", mode='a', index=False, sep=\";\")\n",
    "            print(f\"‚ö†Ô∏è Skipped documents saved to 'skipped_documents.csv' ({len(skipped_documents)} records)\")\n",
    "        else:\n",
    "            print(\"No skipped documents!\")\n",
    "        print(f\"üîö Primary documents processed!\")\n",
    "\n",
    "        # Process all additional versions (added later)\n",
    "        versions_processed = 0  # Counter for processed versions\n",
    "\n",
    "        for future in as_completed(additional_futures):\n",
    "            version_doc_id = future_to_doc[future]\n",
    "            try:\n",
    "                version_doc_id, html = future.result()\n",
    "                if html is None:\n",
    "                    skipped_versions.append({\"doc_id\": version_doc_id, \"reason\": \"No HTML response\"})\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                data_df = extract_expertise_data(soup, all_columns)\n",
    "\n",
    "                # Use cache instead of calling extract_doc_versions again\n",
    "                version_info = versions_cache.get(version_doc_id, {})\n",
    "                data_df[\"doc_id\"] = version_doc_id\n",
    "                data_df[\"date\"] = version_info.get(\"date\", \"Unknown\")\n",
    "                data_df[\"version\"] = version_info.get(\"version\", \"Unknown\")\n",
    "\n",
    "                buffer.append(data_df)\n",
    "                documents_parsed += 1\n",
    "                versions_processed += 1  # Increment processed versions counter\n",
    "\n",
    "                # Display progress\n",
    "                if len(buffer) >= save_interval:\n",
    "                    result_df = pd.concat(buffer, ignore_index=True)\n",
    "                    result_df.to_csv(output_file, mode='a', index=False, header=False, sep=\";\")\n",
    "                    buffer = []\n",
    "                    print(f\"‚úÖ Versions processed: {versions_processed}/{versions_to_process}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped_versions.append({\"doc_id\": version_doc_id, \"reason\": str(e)})\n",
    "                print(f\"‚ö†Ô∏è Error processing version_doc_id {version_doc_id}: {e}\")\n",
    "\n",
    "        # Final save\n",
    "        if buffer:\n",
    "            result_df = pd.concat(buffer, ignore_index=True)\n",
    "            result_df.to_csv(output_file, mode='a', index=False, header=False, sep=\";\")\n",
    "            print(f\"‚úÖ Final save of {len(buffer)} records.\")\n",
    "\n",
    "        # Save skipped versions to CSV\n",
    "        if skipped_versions:\n",
    "            skipped_df = pd.DataFrame(skipped_versions)\n",
    "            skipped_df.to_csv(\"skipped_versions.csv\", mode='a', index=False, sep=\";\")\n",
    "            print(f\"‚ö†Ô∏è Skipped versions saved to 'skipped_versions.csv' ({len(skipped_versions)} records)\")\n",
    "\n",
    "        print(f\"üîö Completed! Skipped versions: {len(skipped_versions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of calling the parse_documents function to parse and save data\n",
    "# parse_documents(\"documents.csv\", all_columns, max_workers=10, save_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING\n",
    "This section defines the steps for cleaning the parsed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parsed data from the CSV file\n",
    "df = pd.read_csv(\"parsed_documents.csv\", sep=\";\", encoding=\"utf-8\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate document IDs\n",
    "df.duplicated(subset=[\"doc_id\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate document IDs, keeping the last occurrence\n",
    "df.drop_duplicates(subset=[\"doc_id\"], keep=\"last\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_mapping = {\n",
    "#     \"document_type\": \"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç—É\",\n",
    "#     \"registration_number_edessb\": \"–†–µ—î—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π –Ω–æ–º–µ—Ä –≤ –Ñ–î–ï–°–°–ë\",\n",
    "#     \"document_version\": \"–í–µ—Ä—Å—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç—É\",\n",
    "#     \"registration_status\": \"–°—Ç–∞—Ç—É—Å —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó\",\n",
    "#     \"document_status\": \"–°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç—É\",\n",
    "#     \"version_full_text\": \"version\",\n",
    "#     \"date_of_version\": \"date\",\n",
    "#     \"document_name\": \"–î–æ–∫—É–º–µ–Ω—Ç\",\n",
    "#     \"issuing_body_expertise\": \"–û—Ä–≥–∞–Ω, —â–æ –≤–∏–¥–∞–≤_–µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∞\",\n",
    "#     \"issuing_body_project_documentation\": \"–û—Ä–≥–∞–Ω, —â–æ –≤–∏–¥–∞–≤_–ü–ö–î\",\n",
    "#     \"object_name\": \"–ù–∞–∑–≤–∞ –æ–±‚Äô—î–∫—Ç–∞\",\n",
    "#     \"project_code\": \"–ö–æ–¥ –ø—Ä–æ–µ–∫—Ç–Ω–æ—ó –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó\",\n",
    "#     \"expertise_area\": \"–ù–∞–ø—Ä—è–º –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏\",\n",
    "#     \"chief_project_expert\": \"–ì–æ–ª–æ–≤–Ω–∏–π –µ–∫—Å–ø–µ—Ä—Ç –ø—Ä–æ–µ–∫—Ç—É\",\n",
    "#     \"registration_number\": \"–†–µ—î—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π –Ω–æ–º–µ—Ä\",\n",
    "#     \"construction_type\": \"–í–∏–¥ –±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–∞\",\n",
    "#     \"project_works_list\": \"–ü–µ—Ä–µ–ª—ñ–∫ –≤–∏–¥—ñ–≤ —Ä–æ–±—ñ—Ç, –≤–∏–∫–æ–Ω–∞–Ω–∏—Ö –ø—Ä–æ–µ–∫—Ç—É–≤–∞–ª—å–Ω–∏–∫–æ–º (–≥–µ–Ω–ø—Ä–æ–µ–∫—Ç—É–≤–∞–ª—å–Ω–∏–∫–æ–º)\",\n",
    "#     \"number_of_design_stages\": \"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–∞–¥—ñ–π –ø—Ä–æ–µ–∫—Ç—É–≤–∞–Ω–Ω—è\",\n",
    "#     \"current_design_stage\": \"–ü–æ—Ç–æ—á–Ω–∞ —Å—Ç–∞–¥—ñ—è –ø—Ä–æ–µ–∫—Ç—É–≤–∞–Ω–Ω—è\",\n",
    "#     \"dkbs_code\": \"–ö–æ–¥ –î–ö–ë–°\",\n",
    "#     \"public_funding_involved\": \"–û–±'—î–∫—Ç —Å–ø–æ—Ä—É–¥–∂—É—é—Ç—å—Å—è —ñ–∑ –∑–∞–ª—É—á–µ–Ω–Ω—è–º –±—é–¥–∂–µ—Ç–Ω–∏—Ö –∫–æ—à—Ç—ñ–≤, –∫–æ—à—Ç—ñ–≤ –¥–µ—Ä–∂–∞–≤–Ω–∏—Ö —ñ –∫–æ–º—É–Ω–∞–ª—å–Ω–∏—Ö –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤, —É—Å—Ç–∞–Ω–æ–≤ —Ç–∞ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π, –∞ —Ç–∞–∫–æ–∂ –∫—Ä–µ–¥–∏—Ç—ñ–≤, –Ω–∞–¥–∞–Ω–∏—Ö –ø—ñ–¥ –¥–µ—Ä–∂–∞–≤–Ω—ñ –≥–∞—Ä–∞–Ω—Ç—ñ—ó?\",\n",
    "#     \"intellectual_property_rights\": \"–ù–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º–∞–π–Ω–æ–≤–∏—Ö –ø—Ä–∞–≤ –Ω–∞ –ø—Ä–æ–µ–∫—Ç–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é (–ø—Ä–∞–≤–æ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –ø—Ä–æ–µ–∫—Ç–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é)\",\n",
    "#     \"contract_date\": \"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä—É –Ω–∞ —Ä–æ–∑—Ä–æ–±–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó\",\n",
    "#     \"project_documentation_number\": \"–ù–æ–º–µ—Ä –ø—Ä–æ–µ–∫—Ç–Ω–æ—ó –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó\",\n",
    "#     \"document_internal_status\": \"–°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞\",\n",
    "#     \"estimate_code\": \"–ö–æ–¥ –∫–æ—à—Ç–æ—Ä–∏—Å—É\",\n",
    "#     \"estimate_version\": \"–ù–æ–º–µ—Ä —Ä–µ–¥–∞–∫—Ü—ñ—ó –∫–æ—à—Ç–æ—Ä–∏—Å—É\",\n",
    "#     \"estimate_date\": \"–î–∞—Ç–∞ –∫–æ—à—Ç–æ—Ä–∏—Å—É\",\n",
    "#     \"declared_estimated_cost\": \"–ó–∞—è–≤–ª–µ–Ω–∞ –∫–æ—à—Ç–æ—Ä–∏—Å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å, —Ç–∏—Å. –≥—Ä–Ω.\",\n",
    "#     \"client_legal_status\": \"–ü—Ä–∞–≤–æ–≤–∏–π —Å—Ç–∞—Ç—É—Å –∑–∞–º–æ–≤–Ω–∏–∫–∞\",\n",
    "#     \"client_name\": \"–ù–∞–∑–≤–∞ –∑–∞–º–æ–≤–Ω–∏–∫–∞\",\n",
    "#     \"client_edrpou\": \"–Ñ–î–†–ü–û–£ –∑–∞–º–æ–≤–Ω–∏–∫–∞\",\n",
    "#     \"doc_id\": \"doc_id\"\n",
    "# }\n",
    "\n",
    "# df_columns = pd.DataFrame([\n",
    "#     {\"english_name\": k, \"ukrainian_name\": v}\n",
    "#     for k, v in column_mapping.items()\n",
    "# ])\n",
    "\n",
    "# df_columns.to_csv(\"column_name_mapping.csv\", index=False, sep=\";\", encoding=\"utf-8\")\n",
    "# print(\"‚úÖ –§–∞–π–ª 'column_name_mapping.csv' —Å—Ç–≤–æ—Ä–µ–Ω–æ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the column mapping from the CSV file\n",
    "column_mapping_df = pd.read_csv(\"column_name_mapping.csv\", delimiter=\";\")\n",
    "column_mapping = dict(zip(column_mapping_df['ukrainian_name'], column_mapping_df['english_name']))\n",
    "\n",
    "# Rename the columns of the dataframe\n",
    "df.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_version\n",
       "‚Ññ1     66609\n",
       "‚Ññ2     11127\n",
       "‚Ññ3      2913\n",
       "‚Ññ4       827\n",
       "‚Ññ5       286\n",
       "‚Ññ6        98\n",
       "‚Ññ7        39\n",
       "‚Ññ8        18\n",
       "‚Ññ9         7\n",
       "‚Ññ10        6\n",
       "‚Ññ11        2\n",
       "‚Ññ13        1\n",
       "‚Ññ12        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value counts of the document_version column\n",
    "df['document_version'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document_name column into document_number and document_date\n",
    "df[['document_number', 'document_date']] = df['document_name'].str.split(' –≤—ñ–¥ ', expand=True, n=1)\n",
    "# Drop the original document_name column\n",
    "df.drop(columns=['document_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "# df.to_csv(\"expertise_english_columns.csv\", index=False, sep=\";\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
