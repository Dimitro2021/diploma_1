{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e88148",
   "metadata": {},
   "source": [
    "# Expertise Organizations Page Parser\n",
    "This notebook is designed to parse and extract data about organizations from the e-construction.gov.ua website. It includes functions for fetching, parsing, and saving data in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for HTTP requests, HTML parsing, and data manipulation.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the page to be parsed.\n",
    "url = 'https://e-construction.gov.ua/organizations/org_type=1/page=5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up HTTP headers for the request to mimic a browser.\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept-Language\": \"uk-UA,uk;q=0.9\"\n",
    "}\n",
    "\n",
    "# Send a GET request to the URL and check the response status.\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the HTML content of the response.\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract data from the parsed HTML content.\n",
    "def get_data(soup):\n",
    "    \"\"\"\n",
    "    Extracts organization data from the parsed HTML content.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): Parsed HTML content.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing organization data.\n",
    "    \"\"\"\n",
    "    # Find all organization blocks.\n",
    "    org_blocks = soup.find_all(\"div\", class_=\"dataset__item\")\n",
    "\n",
    "    # Extract information from each block.\n",
    "    data = []\n",
    "    for block in org_blocks:\n",
    "        name_tag = block.find(\"h3\", class_=\"opendata__name\")\n",
    "        name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "        legal_status = block.find(string=\"Правовий статус\")\n",
    "        status = legal_status.find_next(\"span\").get_text(strip=True) if legal_status else None\n",
    "\n",
    "        edrpou = block.find(string=\"Код ЄДРПОУ\")\n",
    "        edrpou_code = edrpou.find_next(\"span\").get_text(strip=True) if edrpou else None\n",
    "\n",
    "        people = block.find(string=\"Кількість атестованих осіб\")\n",
    "        certified_people = people.find_next(\"span\").get_text(strip=True) if people else None\n",
    "\n",
    "        consequences = block.find(string=\"Класи наслідків\")\n",
    "        consequence_classes = [label.get_text(strip=True) for label in consequences.find_next(\"span\").find_all(\"span\")] if consequences else []\n",
    "\n",
    "        data.append({\n",
    "            \"Назва\": name,\n",
    "            \"Правовий статус\": status,\n",
    "            \"ЄДРПОУ\": edrpou_code,\n",
    "            \"Кількість осіб\": certified_people,\n",
    "            \"Класи наслідків\": consequence_classes\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the get_data function to extract data from the parsed HTML content.\n",
    "get_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa952a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to parse organization data from multiple pages.\n",
    "def parse_organizations_all_pages(pages=12, delay=1, retries=3):\n",
    "    \"\"\"\n",
    "    Parses organization data from multiple pages of the website.\n",
    "    \n",
    "    Args:\n",
    "        pages (int): Number of pages to parse.\n",
    "        delay (int): Delay between requests in seconds.\n",
    "        retries (int): Number of retries for failed requests.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the parsed data.\n",
    "    \"\"\"\n",
    "    base_url = \"https://e-construction.gov.ua/organizations/org_type=1\"\n",
    "    data = []\n",
    "\n",
    "    for page_num in range(1, pages + 1):\n",
    "        url = base_url if page_num == 1 else f\"{base_url}/page={page_num}\"\n",
    "\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                break  # Exit retry loop if successful.\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Attempt {attempt} failed for page {page_num} — {e}\")\n",
    "                if attempt == retries:\n",
    "                    print(f\"❌ Skipping page {page_num} after {retries} failed attempts.\")\n",
    "                    response = None\n",
    "\n",
    "        if not response:\n",
    "            continue  # Skip to the next page.\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        org_blocks = soup.find_all(\"div\", class_=\"dataset__item\")\n",
    "\n",
    "        for block in org_blocks:\n",
    "            name_tag = block.find(\"h3\", class_=\"opendata__name\")\n",
    "            name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "            legal_status = block.find(string=\"Правовий статус\")\n",
    "            status = legal_status.find_next(\"span\").get_text(strip=True) if legal_status else None\n",
    "\n",
    "            edrpou = block.find(string=\"Код ЄДРПОУ\")\n",
    "            edrpou_code = edrpou.find_next(\"span\").get_text(strip=True) if edrpou else None\n",
    "\n",
    "            people = block.find(string=\"Кількість атестованих осіб\")\n",
    "            certified_people = people.find_next(\"span\").get_text(strip=True) if people else None\n",
    "\n",
    "            consequences = block.find(string=\"Класи наслідків\")\n",
    "            consequence_classes = [label.get_text(strip=True) for label in consequences.find_next(\"span\").find_all(\"span\")] if consequences else []\n",
    "\n",
    "            link_tag = block.find(\"a\", class_=\"opendata__link\")\n",
    "            link = link_tag['href'] if link_tag and link_tag.has_attr('href') else None\n",
    "\n",
    "            data.append({\n",
    "                \"Назва\": name,\n",
    "                \"Правовий статус\": status,\n",
    "                \"ЄДРПОУ\": edrpou_code,\n",
    "                \"Кількість осіб\": certified_people,\n",
    "                \"Класи наслідків\": consequence_classes,\n",
    "                \"Посилання\": link\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2466502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the parse_organizations_all_pages function.\n",
    "# df_1 = parse_organizations_all_pages()\n",
    "# df_2 = parse_organizations_all_pages()\n",
    "# df_3 = parse_organizations_all_pages()\n",
    "# df_4 = parse_organizations_all_pages()\n",
    "# df_5 = parse_organizations_all_pages()\n",
    "# df_6 = parse_organizations_all_pages()\n",
    "\n",
    "# Since pages are unstable and change during navigation, the dataset needs to be parsed multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all parsed DataFrames and check for unique organization names.\n",
    "df_all = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6], ignore_index=True)\n",
    "df_all['Назва'].nunique() # Check if there are 128 unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries based on the 'Назва' column.\n",
    "df_all.drop_duplicates(\n",
    "    subset='Назва',\n",
    "    inplace=True,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to CSV and Excel formats.\n",
    "df_all.to_csv('expertise_organisation.csv')\n",
    "df_all.to_excel('expertise_organisation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved CSV file for further analysis.\n",
    "df = pd.read_csv('expertise_organisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa82ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the loaded DataFrame.\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
